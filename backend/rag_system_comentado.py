"""
ü§ñ SISTEMA RAG CON DEEPSEEK V3 - C√ìDIGO COMPLETAMENTE DOCUMENTADO
================================================================

Este archivo contiene el sistema completo de RAG (Retrieval-Augmented Generation)
con integraci√≥n de DeepSeek V3 para an√°lisis acad√©mico contextual.

TECNOLOG√çAS CLAVE:
- DeepSeek V3: Modelo de 671B par√°metros v√≠a OpenRouter API
- all-mpnet-base-v2: Embeddings de 768 dimensiones
- ChromaDB: Base de datos vectorial con √≠ndice HNSW
- OpenRouter API: Gateway para modelos de IA avanzados
- Django REST Framework: Backend API
- React: Frontend interactivo

FLUJO RAG COMPLETO:
1. QUERY ENCODING: Convierte pregunta del usuario a vector 768D
2. SIMILARITY SEARCH: Busca chunks relevantes en ChromaDB
3. CONTEXT PREPARATION: Prepara contexto para DeepSeek V3
4. LLM ANALYSIS: DeepSeek V3 analiza y genera respuesta acad√©mica
5. RESPONSE FORMATTING: Estructura respuesta con fuentes y metadata
"""

import os
import re
import sys
import logging
from typing import List, Dict, Any
from pathlib import Path

# === CONFIGURACI√ìN DE LOGGING ===
# Sistema de logging para monitorear operaciones RAG
logger = logging.getLogger(__name__)

# === CONFIGURACI√ìN DE MODELOS LLM ===
# Sistema de fallback jer√°rquico para modelos de generaci√≥n
USE_LLM = os.environ.get("USE_LLM", "false").lower() == "true"
LLM_MODEL = os.environ.get("LLM_MODEL", "Qwen/Qwen1.5-1.8B-Chat")
LLM_MAX_NEW_TOKENS = int(os.environ.get("LLM_MAX_NEW_TOKENS", "240"))
_LLM_PIPELINE = None  # Cache global del pipeline de generaci√≥n

def initialize_rag_components():
    """
    Inicializa todos los componentes del sistema RAG
    
    Esta funci√≥n es el punto de entrada principal que configura:
    1. Modelo de embeddings (all-mpnet-base-v2)
    2. Cliente ChromaDB con persistencia
    3. Colecci√≥n de documentos vectorizados
    
    DETALLES T√âCNICOS:
    - all-mpnet-base-v2: Modelo basado en MPNet de Microsoft
    - 768 dimensiones: Tama√±o √≥ptimo para similitud sem√°ntica
    - ChromaDB: Base vectorial con √≠ndice HNSW para b√∫squeda eficiente
    - Persistencia local: Datos almacenados en disco para velocidad
    
    Returns:
        tuple: (modelo_embeddings, cliente_chromadb, colecci√≥n)
    
    Raises:
        Exception: Si falla la inicializaci√≥n de componentes cr√≠ticos
    """
    try:
        # === IMPORTACI√ìN DE BIBLIOTECAS CR√çTICAS ===
        from sentence_transformers import SentenceTransformer
        import chromadb

        # === CONFIGURACI√ìN DE RUTAS ===
        # Detectar autom√°ticamente la ubicaci√≥n de ChromaDB
        current_dir = Path(__file__).parent
        chroma_path = current_dir.parent.parent / "chroma_db_simple"
        
        logger.info(f"üìÅ Ruta ChromaDB: {chroma_path}")
        
        # === INICIALIZACI√ìN DEL MODELO DE EMBEDDINGS ===
        # all-mpnet-base-v2: Modelo de embeddings de alta calidad
        # - Entrenado en 1B+ pares de oraciones
        # - Soporte multilenguaje (espa√±ol incluido)
        # - Optimizado para similitud sem√°ntica
        # - 768 dimensiones (balance entre calidad y eficiencia)
        logger.info("ü§ñ Cargando modelo all-mpnet-base-v2...")
        model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")
        
        # Verificar dimensiones del modelo
        embedding_dim = model.get_sentence_embedding_dimension()
        logger.info(f"üìä Dimensiones de embeddings: {embedding_dim}")

        # === INICIALIZACI√ìN DE CHROMADB ===
        # ChromaDB: Base de datos vectorial optimizada
        # - √çndice HNSW: Hierarchical Navigable Small World
        # - B√∫squeda aproximada en O(log n)
        # - Soporte para metadata estructurada
        # - Backend SQLite para persistencia
        logger.info("üóÑÔ∏è Inicializando cliente ChromaDB...")
        client = chromadb.PersistentClient(path=str(chroma_path))
        
        # Obtener colecci√≥n de documentos
        collection = client.get_collection("simple_rag_docs")
        
        # === VERIFICACI√ìN DE ESTADO ===
        # Contar documentos disponibles para b√∫squeda
        count = collection.count()
        logger.info(f"üìö Documentos en ChromaDB: {count}")
        
        if count == 0:
            logger.warning("‚ö†Ô∏è ChromaDB est√° vac√≠a. Ejecutar ETL primero.")
        
        logger.info("‚úÖ Componentes RAG inicializados correctamente")
        return model, client, collection

    except Exception as e:
        logger.error(f"‚ùå Error inicializando componentes RAG: {str(e)}")
        raise

def perform_semantic_search(query: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """
    Ejecuta b√∫squeda sem√°ntica en la base de datos vectorial
    
    ALGORITMO DE B√öSQUEDA:
    1. ENCODING: Convierte la query a vector de 768 dimensiones
    2. SIMILARITY: Calcula similitud coseno con todos los documentos
    3. RANKING: Ordena resultados por score de similitud
    4. FILTERING: Retorna top_k resultados m√°s relevantes
    
    OPTIMIZACIONES:
    - √çndice HNSW para b√∫squeda sub-lineal
    - Normalizaci√≥n L2 de vectores para similitud coseno
    - Filtrado por threshold de relevancia
    - Preservaci√≥n de metadata para trazabilidad
    
    Args:
        query (str): Pregunta del usuario en lenguaje natural
        top_k (int): N√∫mero m√°ximo de resultados a retornar
        
    Returns:
        List[Dict]: Lista de chunks relevantes con metadata
        
    Estructura de retorno:
    [
        {
            'texto': 'contenido del chunk...',
            'archivo': 'documento_fuente.pdf',
            'chunk': 'chunk_001',
            'similarity_score': 0.85
        },
        ...
    ]
    """
    
    # === PRIORIZACI√ìN DE SISTEMAS ===
    # El sistema prioriza diferentes implementaciones RAG:
    # 1. Sistema optimizado (h√≠brido + reranking)
    # 2. Sistema mejorado (con APIs externas)
    # 3. Sistema b√°sico (solo ChromaDB)
    
    use_optimized = os.getenv("USE_OPTIMIZED_RAG", "true").lower() == "true"
    
    if use_optimized:
        try:
            from .optimized_rag_system import perform_optimized_search
            logger.info("üöÄ Usando sistema RAG optimizado (h√≠brido + reranking)")
            return perform_optimized_search(query, top_k)
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è Sistema optimizado no disponible: {e}")
        except Exception as e:
            logger.error(f"‚ùå Error en sistema optimizado: {e}, fallback a sistema cl√°sico")
    
    # === B√öSQUEDA B√ÅSICA CON CHROMADB ===
    try:
        # Inicializar componentes RAG
        model, client, collection = initialize_rag_components()

        # === FASE 1: VERIFICACI√ìN DE DATOS ===
        try:
            # Verificar que hay documentos para buscar
            count = collection.count()
            if count == 0:
                logger.warning("‚ö†Ô∏è Colecci√≥n 'simple_rag_docs' est√° vac√≠a")
                return []
            logger.info(f"üîç Buscando en colecci√≥n con {count} documentos")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è No se pudo verificar el conteo: {e}")
            # Continuar con la b√∫squeda de todas formas

        # === FASE 2: ENCODING DE LA QUERY ===
        logger.info(f"üî§ Encoding query: '{query[:50]}...'")
        
        # Generar embedding de la query usando el mismo modelo
        # que se us√≥ para los documentos (consistencia crucial)
        query_embedding = model.encode([query])
        
        logger.info(f"üéØ Query embedding generado: {query_embedding.shape}")

        # === FASE 3: B√öSQUEDA POR SIMILITUD ===
        logger.info(f"üîç Ejecutando b√∫squeda sem√°ntica (top_k={top_k})")
        
        # B√∫squeda en ChromaDB usando similitud coseno
        # ChromaDB maneja autom√°ticamente:
        # - Normalizaci√≥n de vectores
        # - C√°lculo de similitud coseno
        # - Ranking por score
        # - Filtrado por top_k
        results = collection.query(
            query_embeddings=query_embedding.tolist(),
            n_results=top_k,
            include=['documents', 'metadatas', 'distances']
        )

        # === FASE 4: PROCESAMIENTO DE RESULTADOS ===
        formatted_results = []
        
        if results and results['documents'] and results['documents'][0]:
            documents = results['documents'][0]
            metadatas = results['metadatas'][0] if results['metadatas'] else []
            distances = results['distances'][0] if results['distances'] else []
            
            logger.info(f"üìä Encontrados {len(documents)} resultados")
            
            # Procesar cada resultado
            for i, doc in enumerate(documents):
                # Obtener metadata (con fallbacks para robustez)
                metadata = metadatas[i] if i < len(metadatas) else {}
                distance = distances[i] if i < len(distances) else 1.0
                
                # Convertir distancia a score de similitud
                # ChromaDB retorna distancias (menor = m√°s similar)
                # Convertimos a score de similitud (mayor = m√°s similar)
                similarity_score = 1.0 - distance if distance <= 1.0 else 0.0
                
                # Estructurar resultado
                result = {
                    'texto': doc,
                    'archivo': metadata.get('source_file', 'documento_desconocido'),
                    'chunk': metadata.get('chunk_id', f'chunk_{i}'),
                    'similarity_score': similarity_score,
                    'metadata': metadata  # Metadata completa para uso avanzado
                }
                
                formatted_results.append(result)
                
                # Log detallado para debugging
                logger.debug(f"   üìÑ {result['archivo']} - Score: {similarity_score:.3f}")
        
        else:
            logger.warning("‚ö†Ô∏è No se encontraron resultados para la query")
        
        logger.info(f"‚úÖ B√∫squeda completada: {len(formatted_results)} resultados")
        return formatted_results
        
    except Exception as e:
        logger.error(f"‚ùå Error en b√∫squeda sem√°ntica: {str(e)}")
        return []

def generate_rag_response(query: str, search_results: List[Dict[str, Any]]) -> str:
    """
    Genera respuesta RAG usando DeepSeek V3 para an√°lisis acad√©mico
    
    JERARQU√çA DE MODELOS:
    1. DeepSeek V3 (671B par√°metros) - v√≠a OpenRouter API
    2. Sistema b√°sico estructurado (fallback)
    
    DEEPSEEK V3 CARACTER√çSTICAS:
    - 671 billion par√°metros
    - Contexto de 128K tokens
    - Optimizado para an√°lisis acad√©mico
    - Multimodal (texto, c√≥digo, matem√°ticas)
    - Temperature 0.3 para consistencia
    
    PROMPT ENGINEERING:
    - Rol espec√≠fico: Asistente acad√©mico
    - Contexto estructurado con fuentes numeradas
    - Instrucciones claras para citas y referencias
    - Formato de respuesta acad√©mica
    
    Args:
        query (str): Pregunta del usuario
        search_results (List[Dict]): Resultados de b√∫squeda sem√°ntica
        
    Returns:
        str o Dict: Respuesta generada con fuentes (formato depende del fallback)
    """
    
    # === VALIDACI√ìN DE ENTRADA ===
    if not search_results:
        logger.warning("‚ö†Ô∏è No hay resultados de b√∫squeda para generar respuesta")
        return _generate_no_results_response(query)

    # === PRIORIDAD 1: DEEPSEEK V3 VIA OPENROUTER ===
    from dotenv import load_dotenv
    load_dotenv()
    
    openrouter_key = os.getenv("OPENROUTER_API_KEY")
    
    if openrouter_key:
        logger.info("ü§ñ Usando DeepSeek V3 para an√°lisis acad√©mico")
        
        try:
            import requests
            import json
            
            # === PREPARACI√ìN DEL CONTEXTO ===
            # Formatear chunks de b√∫squeda como contexto estructurado
            context_chunks = []
            for i, result in enumerate(search_results[:5], 1):  # Top 5 resultados
                # Formato: [FUENTE N: archivo.pdf] contenido
                chunk = f"[FUENTE {i}: {result['archivo']}]\n{result['texto']}"
                context_chunks.append(chunk)
            
            # Unir todos los chunks en contexto √∫nico
            context_text = "\n\n".join(context_chunks)
            
            # === PROMPT ENGINEERING PARA DEEPSEEK V3 ===
            # Prompt especializado para an√°lisis acad√©mico
            prompt = f"""Eres un asistente acad√©mico especializado en an√°lisis de textos universitarios. 
Tu tarea es proporcionar respuestas precisas, detalladas y bien fundamentadas bas√°ndote en los documentos proporcionados.

CONTEXTO ACAD√âMICO:
{context_text}

PREGUNTA DEL ESTUDIANTE:
{query}

INSTRUCCIONES:
- Proporciona una respuesta acad√©mica completa y detallada
- Cita espec√≠ficamente los autores y a√±os cuando sea relevante
- Incluye referencias directas a los textos
- Mant√©n un tono acad√©mico y profesional
- Si la informaci√≥n no est√° en el contexto, ind√≠calo claramente
- Usa el formato [FUENTE N] para referenciar las fuentes

RESPUESTA:"""

            # === CONFIGURACI√ìN DE LA API ===
            # Headers requeridos por OpenRouter
            headers = {
                "Authorization": f"Bearer {openrouter_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "http://localhost:8000",  # Identificaci√≥n de la app
                "X-Title": "RAG Academic Assistant"
            }
            
            # === PAR√ÅMETROS DEL MODELO ===
            # Configuraci√≥n optimizada para an√°lisis acad√©mico
            data = {
                "model": "deepseek/deepseek-v3",    # Modelo espec√≠fico
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.3,    # Baja temperatura para consistencia
                "max_tokens": 2000,    # Suficiente para an√°lisis detallado
                "top_p": 0.9,         # Control de diversidad
                "frequency_penalty": 0.1,  # Reducir repetici√≥n
                "presence_penalty": 0.1    # Fomentar nuevas ideas
            }
            
            # === LLAMADA A LA API ===
            logger.info("üåê Enviando request a OpenRouter/DeepSeek V3...")
            
            response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=60  # Timeout de 60 segundos
            )
            
            # === PROCESAMIENTO DE LA RESPUESTA ===
            if response.status_code == 200:
                result = response.json()
                
                # Verificar estructura de respuesta v√°lida
                if 'choices' in result and len(result['choices']) > 0:
                    deepseek_response = result['choices'][0]['message']['content']
                    
                    # M√©tricas de la respuesta
                    response_length = len(deepseek_response)
                    logger.info(f"‚úÖ DeepSeek V3 respuesta exitosa: {response_length} caracteres")
                    
                    # Log de uso de tokens (si est√° disponible)
                    if 'usage' in result:
                        usage = result['usage']
                        logger.info(f"üìä Tokens: prompt={usage.get('prompt_tokens', 0)}, "
                                  f"completion={usage.get('completion_tokens', 0)}")
                    
                    return deepseek_response
                else:
                    logger.error("‚ùå Respuesta de DeepSeek V3 sin contenido v√°lido")
            else:
                logger.error(f"‚ùå Error API OpenRouter: {response.status_code} - {response.text}")
                
        except requests.exceptions.Timeout:
            logger.error("‚ùå Timeout en llamada a DeepSeek V3")
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Error de conexi√≥n con OpenRouter: {e}")
        except Exception as e:
            logger.error(f"‚ùå Error procesando DeepSeek V3: {e}")

    # === FALLBACK: SISTEMA B√ÅSICO ESTRUCTURADO ===
    logger.info("üîÑ Usando sistema de respuesta estructurado (fallback)")
    
    try:
        # Preparar contexto para sistema b√°sico
        context_chunks = []
        sources_info = []
        
        # Procesar resultados de b√∫squeda
        for i, result in enumerate(search_results[:5], 1):
            # Chunk con metadata para contexto
            chunk_with_metadata = f"[FUENTE {i}: {result['archivo']}]\n{result['texto']}"
            context_chunks.append(chunk_with_metadata)
            
            # Informaci√≥n de fuentes para frontend
            sources_info.append({
                "numero": i,
                "archivo": result['archivo'],
                "chunk": result['chunk'],
                "similarity_score": result['similarity_score'],
                "texto_preview": result['texto'][:200] + "..." if len(result['texto']) > 200 else result['texto']
            })
        
        logger.info(f"üîß Generando respuesta b√°sica para: '{query[:50]}...' con {len(context_chunks)} chunks")
        
        # === GENERACI√ìN DE RESPUESTA B√ÅSICA ===
        response = _generate_advanced_fallback_response(query, search_results)
        
        # Verificar calidad de respuesta
        if response and len(response.strip()) > 50:
            # Estructurar respuesta con fuentes
            structured_response = {
                "respuesta": clean_and_format_response(response),
                "fuentes": sources_info,
                "total_fuentes": len(sources_info),
                "query_procesada": query
            }
            
            logger.info(f"‚úÖ Respuesta b√°sica generada: {len(response)} caracteres con {len(sources_info)} fuentes")
            return structured_response
        else:
            logger.warning("‚ö†Ô∏è Respuesta b√°sica insuficiente, usando fallback final")
            return _generate_advanced_fallback_response_with_sources(query, search_results)
        
    except Exception as e:
        logger.error(f"‚ùå Error en sistema b√°sico: {e}")
        return _generate_advanced_fallback_response_with_sources(query, search_results)

def _generate_no_results_response(query: str) -> str:
    """
    Genera respuesta cuando no se encuentran documentos relevantes
    
    Esta funci√≥n maneja el caso donde la b√∫squeda sem√°ntica no retorna
    resultados suficientemente relevantes para la query del usuario.
    
    Args:
        query (str): Query original del usuario
        
    Returns:
        str: Mensaje explicativo para el usuario
    """
    return (
        f"No encontr√© informaci√≥n espec√≠fica sobre '{query}' en los documentos disponibles. "
        f"Te sugiero reformular la pregunta o verificar que los documentos contengan "
        f"informaci√≥n relacionada con el tema consultado.\n\n"
        f"Algunas sugerencias:\n"
        f"- Usa t√©rminos m√°s espec√≠ficos o sin√≥nimos\n"
        f"- Verifica la ortograf√≠a de nombres propios\n"
        f"- Intenta con preguntas m√°s generales sobre el tema"
    )

def _generate_advanced_fallback_response_with_sources(query: str, search_results: List[Dict[str, Any]]) -> Dict:
    """
    Fallback avanzado que incluye fuentes estructuradas
    
    Esta funci√≥n genera una respuesta estructurada cuando fallan
    los sistemas principales de generaci√≥n de respuestas.
    
    Args:
        query (str): Query del usuario
        search_results (List[Dict]): Resultados de b√∫squeda
        
    Returns:
        Dict: Respuesta estructurada con fuentes
    """
    # Generar respuesta fallback
    fallback_response = _generate_advanced_fallback_response(query, search_results)
    
    # Preparar informaci√≥n de fuentes
    sources_info = []
    for i, result in enumerate(search_results[:5], 1):
        sources_info.append({
            "numero": i,
            "archivo": result['archivo'],
            "chunk": result['chunk'],
            "similarity_score": result['similarity_score'],
            "texto_preview": result['texto'][:200] + "..." if len(result['texto']) > 200 else result['texto']
        })
    
    return {
        "respuesta": fallback_response,
        "fuentes": sources_info,
        "total_fuentes": len(sources_info),
        "query_procesada": query
    }

def _generate_advanced_fallback_response(query: str, search_results: List[Dict[str, Any]]) -> str:
    """
    Genera respuesta avanzada sin usar LLMs externos
    
    ALGORITMO DE RESPUESTA:
    1. AN√ÅLISIS DE QUERY: Identifica t√©rminos clave y patrones
    2. RANKING DE CONTENIDO: Prioriza chunks m√°s relevantes
    3. EXTRACCI√ìN DE CITAS: Identifica referencias acad√©micas
    4. S√çNTESIS: Combina informaci√≥n de m√∫ltiples fuentes
    5. FORMATEO: Estructura respuesta acad√©mica
    
    Args:
        query (str): Query del usuario
        search_results (List[Dict]): Resultados de b√∫squeda
        
    Returns:
        str: Respuesta generada algor√≠tmicamente
    """
    import re
    
    if not search_results:
        return _generate_no_results_response(query)
    
    # === AN√ÅLISIS DE LA QUERY ===
    # Extraer t√©rminos clave de la pregunta
    query_terms = _extract_key_terms(query)
    logger.debug(f"üîç T√©rminos clave extra√≠dos: {query_terms}")
    
    # === AN√ÅLISIS DE CONTENIDO ===
    relevant_passages = []
    academic_references = []
    
    # Procesar cada resultado de b√∫squeda
    for i, result in enumerate(search_results[:3], 1):  # Top 3 para calidad
        text = result['texto']
        source = result['archivo']
        score = result['similarity_score']
        
        # Extraer pasajes m√°s relevantes del chunk
        relevant_parts = _extract_relevant_passages(text, query_terms)
        
        # Buscar referencias acad√©micas en el texto
        references = _extract_academic_references(text)
        
        # Agregar a listas
        for passage in relevant_parts:
            relevant_passages.append({
                'text': passage,
                'source': source,
                'source_number': i,
                'score': score
            })
        
        academic_references.extend(references)
    
    # === CONSTRUCCI√ìN DE RESPUESTA ===
    response_parts = []
    
    # Introducci√≥n contextual
    if query_terms:
        intro = f"Bas√°ndome en los documentos disponibles, puedo proporcionar la siguiente informaci√≥n sobre {', '.join(query_terms[:3])}:"
        response_parts.append(intro)
    
    # Contenido principal
    if relevant_passages:
        response_parts.append("\n")
        
        # Agrupar pasajes por tema/fuente
        for i, passage in enumerate(relevant_passages[:5], 1):
            formatted_passage = f"{i}. {passage['text']} [Fuente: {passage['source']}]"
            response_parts.append(formatted_passage)
    
    # Referencias acad√©micas encontradas
    if academic_references:
        response_parts.append("\n\nReferencias acad√©micas mencionadas:")
        unique_refs = list(set(academic_references))[:3]  # M√°ximo 3 referencias
        for ref in unique_refs:
            response_parts.append(f"- {ref}")
    
    # Conclusi√≥n
    if len(search_results) > 3:
        conclusion = f"\n\nEsta informaci√≥n est√° basada en {len(search_results)} documentos relevantes. Para un an√°lisis m√°s detallado, te recomiendo revisar las fuentes espec√≠ficas mencionadas."
        response_parts.append(conclusion)
    
    # Unir todas las partes
    final_response = "\n".join(response_parts)
    
    # Verificar longitud m√≠nima
    if len(final_response.strip()) < 100:
        # Respuesta muy corta, usar contenido directo
        return _generate_direct_content_response(query, search_results)
    
    return final_response

def _extract_key_terms(query: str) -> List[str]:
    """
    Extrae t√©rminos clave de la query del usuario
    
    ALGORITMO:
    1. Normalizaci√≥n de texto (min√∫sculas, eliminar acentos)
    2. Filtrado de stopwords
    3. Extracci√≥n de entidades (nombres propios, a√±os)
    4. Ranking por relevancia
    
    Args:
        query (str): Query del usuario
        
    Returns:
        List[str]: Lista de t√©rminos clave ordenados por importancia
    """
    # Stopwords en espa√±ol (palabras comunes a filtrar)
    stopwords = {
        'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'es', 'se', 'no', 'te', 'lo', 'le', 'da', 'su', 'por', 'son',
        'con', 'para', 'al', 'del', 'los', 'las', 'una', 'como', 'qu√©', 'cu√°l', 'cu√°les', 'sobre', 'seg√∫n',
        'c√≥mo', 'd√≥nde', 'cu√°ndo', 'por qu√©', 'porque', 'pero', 'sin', 'hasta', 'desde', 'durante'
    }
    
    # Normalizar query
    normalized_query = query.lower()
    
    # Extraer palabras
    words = re.findall(r'\b\w+\b', normalized_query)
    
    # Filtrar stopwords y palabras muy cortas
    key_terms = [word for word in words if word not in stopwords and len(word) > 2]
    
    # Detectar a√±os (importante en contexto acad√©mico)
    years = re.findall(r'\b(19|20)\d{2}\b', query)
    key_terms.extend(years)
    
    # Detectar nombres propios (comienzan con may√∫scula)
    proper_nouns = re.findall(r'\b[A-Z][a-z]+\b', query)
    key_terms.extend([noun.lower() for noun in proper_nouns])
    
    # Remover duplicados manteniendo orden
    seen = set()
    unique_terms = []
    for term in key_terms:
        if term not in seen:
            seen.add(term)
            unique_terms.append(term)
    
    return unique_terms[:10]  # M√°ximo 10 t√©rminos

def _extract_relevant_passages(text: str, query_terms: List[str]) -> List[str]:
    """
    Extrae pasajes relevantes de un texto bas√°ndose en t√©rminos clave
    
    Args:
        text (str): Texto a analizar
        query_terms (List[str]): T√©rminos clave a buscar
        
    Returns:
        List[str]: Lista de pasajes relevantes
    """
    if not query_terms:
        # Si no hay t√©rminos, retornar primeras oraciones
        sentences = re.split(r'[.!?]+', text)
        return [s.strip() for s in sentences[:2] if len(s.strip()) > 20]
    
    # Dividir texto en oraciones
    sentences = re.split(r'[.!?]+', text)
    relevant_sentences = []
    
    # Evaluar cada oraci√≥n
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) < 20:  # Filtrar oraciones muy cortas
            continue
        
        # Contar t√©rminos presentes en la oraci√≥n
        sentence_lower = sentence.lower()
        term_count = sum(1 for term in query_terms if term in sentence_lower)
        
        # Si contiene t√©rminos relevantes, agregar
        if term_count > 0:
            relevant_sentences.append((sentence, term_count))
    
    # Ordenar por relevancia (n√∫mero de t√©rminos)
    relevant_sentences.sort(key=lambda x: x[1], reverse=True)
    
    # Retornar top oraciones (solo el texto)
    return [sentence[0] for sentence in relevant_sentences[:3]]

def _extract_academic_references(text: str) -> List[str]:
    """
    Extrae referencias acad√©micas del texto
    
    PATRONES DETECTADOS:
    - (Autor, 2020)
    - (Autor et al., 2020)
    - Seg√∫n Autor (2020)
    - Como se√±ala Autor et al. (2020)
    
    Args:
        text (str): Texto a analizar
        
    Returns:
        List[str]: Lista de referencias encontradas
    """
    references = []
    
    # Patr√≥n 1: (Autor, a√±o)
    pattern1 = r'\(([A-Z][a-zA-Z]+),?\s+(\d{4})\)'
    matches1 = re.findall(pattern1, text)
    for match in matches1:
        references.append(f"{match[0]} ({match[1]})")
    
    # Patr√≥n 2: (Autor et al., a√±o)
    pattern2 = r'\(([A-Z][a-zA-Z]+)\s+et\s+al\.,?\s+(\d{4})\)'
    matches2 = re.findall(pattern2, text)
    for match in matches2:
        references.append(f"{match[0]} et al. ({match[1]})")
    
    # Patr√≥n 3: Seg√∫n Autor
    pattern3 = r'[Ss]eg√∫n\s+([A-Z][a-zA-Z]+)'
    matches3 = re.findall(pattern3, text)
    for match in matches3:
        references.append(f"Seg√∫n {match}")
    
    return references

def _generate_direct_content_response(query: str, search_results: List[Dict[str, Any]]) -> str:
    """
    Genera respuesta directa usando contenido de los resultados
    
    Args:
        query (str): Query del usuario
        search_results (List[Dict]): Resultados de b√∫squeda
        
    Returns:
        str: Respuesta directa
    """
    if not search_results:
        return _generate_no_results_response(query)
    
    response_parts = ["Bas√°ndome en los documentos encontrados:\n"]
    
    for i, result in enumerate(search_results[:3], 1):
        # Tomar primera parte del texto como respuesta
        text_preview = result['texto'][:300] + "..." if len(result['texto']) > 300 else result['texto']
        source = result['archivo']
        
        response_parts.append(f"{i}. {text_preview} [Fuente: {source}]")
    
    return "\n\n".join(response_parts)

def clean_and_format_response(response: str) -> str:
    """
    Limpia y formatea la respuesta para el usuario final
    
    PROCESAMIENTO:
    1. Normalizaci√≥n de espacios en blanco
    2. Correcci√≥n de puntuaci√≥n
    3. Formato de p√°rrafos
    4. Limpieza de caracteres especiales
    
    Args:
        response (str): Respuesta cruda a limpiar
        
    Returns:
        str: Respuesta limpia y formateada
    """
    if not response:
        return "No se pudo generar una respuesta v√°lida."
    
    # Normalizar espacios en blanco
    response = re.sub(r'\s+', ' ', response)
    response = re.sub(r'\n\s*\n', '\n\n', response)
    
    # Corregir puntuaci√≥n
    response = re.sub(r'\s+([.!?,:;])', r'\1', response)
    response = re.sub(r'([.!?])\s*([A-Z])', r'\1 \2', response)
    
    # Limpiar caracteres especiales problem√°ticos
    response = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', response)
    
    # Asegurar terminaci√≥n correcta
    response = response.strip()
    if response and not response.endswith(('.', '!', '?', ':')):
        response += '.'
    
    return response