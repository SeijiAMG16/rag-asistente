#!/usr/bin/env python3
"""
üîÑ PROCESO ETL COMPLETO PARA RAG ASISTENTE - C√ìDIGO DOCUMENTADO
==============================================================

Este archivo contiene el procesador ETL completo con comentarios detallados
explicando cada funci√≥n y bloque de c√≥digo para entender el funcionamiento
del sistema de embeddings y procesamiento de documentos.

TECNOLOG√çAS UTILIZADAS:
- PyPDF2 / pdfplumber: Extracci√≥n de texto de PDFs
- sentence-transformers: Generaci√≥n de embeddings (all-mpnet-base-v2)
- ChromaDB: Base de datos vectorial para almacenamiento
- PyTorch: Framework de deep learning (backend de transformers)
- SQLite: Backend de persistencia para metadata
- HNSW: Algoritmo de b√∫squeda vectorial eficiente

FLUJO ETL:
1. EXTRACT: Extrae texto de PDFs usando pdfplumber
2. TRANSFORM: Procesa y fragmenta texto en chunks inteligentes
3. LOAD: Genera embeddings 768D y almacena en ChromaDB
"""

import os
import sys
import time
import argparse
from pathlib import Path
from typing import List, Dict, Tuple
import re
import logging

# Configurar logging para monitorear el proceso
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RAGETLProcessor:
    """
    Procesador ETL completo para el sistema RAG
    
    Esta clase maneja todo el pipeline de procesamiento:
    - Extracci√≥n de texto de documentos PDF
    - Transformaci√≥n y limpieza de texto
    - Fragmentaci√≥n inteligente (chunking)
    - Generaci√≥n de embeddings vectoriales
    - Carga en base de datos vectorial ChromaDB
    """
    
    def __init__(self, project_root: Path = None):
        """
        Inicializa el procesador ETL con configuraci√≥n por defecto
        
        Args:
            project_root: Ruta ra√≠z del proyecto (se detecta autom√°ticamente si no se proporciona)
        """
        # === CONFIGURACI√ìN DE DIRECTORIOS ===
        # Detectar autom√°ticamente la ruta del proyecto si no se proporciona
        self.project_root = project_root or Path(__file__).parent.parent
        
        # Definir estructura de directorios del proyecto
        self.data_dir = self.project_root / "data"  # Directorio principal de datos
        self.pdfs_dir = self.data_dir / "pdfs"      # PDFs originales
        self.texts_dir = self.data_dir / "texts"    # Textos extra√≠dos (.txt)
        self.chroma_dir = self.project_root / "chroma_db_simple"  # Base vectorial
        
        # === CONFIGURACI√ìN DE EMBEDDINGS ===
        # Par√°metros para el chunking (fragmentaci√≥n de texto)
        self.chunk_size = 800      # Tama√±o m√°ximo de cada fragmento (tokens)
        self.chunk_overlap = 100   # Solapamiento entre fragmentos (preserva contexto)
        
        # Modelo de embeddings optimizado - all-mpnet-base-v2
        # Este modelo genera vectores de 768 dimensiones con alta calidad sem√°ntica
        self.embedding_model = "sentence-transformers/all-mpnet-base-v2"
        
        # === ESTAD√çSTICAS DEL PROCESO ===
        # Contadores para monitorear el progreso
        self.stats = {
            'pdfs_processed': 0,     # N√∫mero de PDFs procesados
            'texts_created': 0,      # Archivos TXT generados
            'chunks_generated': 0,   # Fragmentos de texto creados
            'embeddings_created': 0, # Vectores de embeddings generados
            'total_time': 0,         # Tiempo total de procesamiento
            'errors': []             # Lista de errores encontrados
        }
        
        logger.info(f"üîß ETL Processor inicializado")
        logger.info(f"üìÅ Directorio de datos: {self.data_dir}")
        logger.info(f"ü§ñ Modelo de embeddings: {self.embedding_model}")
        logger.info(f"üìä Chunk size: {self.chunk_size}, Overlap: {self.chunk_overlap}")

    def create_directories(self):
        """
        Crea la estructura de directorios necesaria para el proyecto
        
        Esta funci√≥n asegura que todos los directorios requeridos existan
        antes de comenzar el procesamiento de documentos.
        """
        directories = [
            self.data_dir,     # data/
            self.pdfs_dir,     # data/pdfs/
            self.texts_dir,    # data/texts/
            self.chroma_dir    # chroma_db_simple/
        ]
        
        for directory in directories:
            try:
                # Crear directorio con parents=True (crea directorios padre si no existen)
                # exist_ok=True evita errores si el directorio ya existe
                directory.mkdir(parents=True, exist_ok=True)
                logger.info(f"üìÅ Directorio asegurado: {directory}")
            except Exception as e:
                error_msg = f"‚ùå Error creando directorio {directory}: {e}"
                logger.error(error_msg)
                self.stats['errors'].append(error_msg)

    def extract_from_pdfs(self) -> bool:
        """
        FASE EXTRACT: Extrae texto de todos los PDFs en el directorio pdfs/
        
        Utiliza pdfplumber como biblioteca principal para extracci√≥n de texto,
        con PyPDF2 como fallback en caso de errores.
        
        Returns:
            bool: True si la extracci√≥n fue exitosa, False en caso de errores cr√≠ticos
        """
        logger.info("üîÑ FASE EXTRACT: Iniciando extracci√≥n de PDFs")
        
        # Verificar que el directorio de PDFs existe
        if not self.pdfs_dir.exists():
            logger.warning(f"‚ö†Ô∏è Directorio de PDFs no existe: {self.pdfs_dir}")
            return False
        
        # Buscar todos los archivos PDF en el directorio
        pdf_files = list(self.pdfs_dir.glob("*.pdf"))
        
        if not pdf_files:
            logger.warning(f"‚ö†Ô∏è No se encontraron archivos PDF en {self.pdfs_dir}")
            return False
        
        logger.info(f"üìÑ Encontrados {len(pdf_files)} archivos PDF para procesar")
        
        # Procesar cada PDF individualmente
        for pdf_file in pdf_files:
            try:
                self._extract_single_pdf(pdf_file)
                self.stats['pdfs_processed'] += 1
                
            except Exception as e:
                error_msg = f"‚ùå Error procesando {pdf_file.name}: {e}"
                logger.error(error_msg)
                self.stats['errors'].append(error_msg)
                continue  # Continuar con el siguiente PDF
        
        logger.info(f"‚úÖ Extracci√≥n completada: {self.stats['pdfs_processed']} PDFs procesados")
        return True

    def _extract_single_pdf(self, pdf_path: Path):
        """
        Extrae texto de un √∫nico archivo PDF
        
        Esta funci√≥n utiliza pdfplumber como m√©todo principal de extracci√≥n
        porque ofrece mejor calidad en la extracci√≥n de texto y manejo de layout.
        
        Args:
            pdf_path: Ruta al archivo PDF a procesar
        """
        logger.info(f"üìñ Procesando: {pdf_path.name}")
        
        # Determinar el nombre del archivo TXT de salida
        txt_filename = pdf_path.stem + ".txt"  # stem = nombre sin extensi√≥n
        txt_path = self.texts_dir / txt_filename
        
        # Si el archivo TXT ya existe, saltar (evitar reprocesamiento)
        if txt_path.exists():
            logger.info(f"‚è≠Ô∏è Archivo TXT ya existe: {txt_filename}")
            return
        
        extracted_text = ""
        
        try:
            # === M√âTODO PRINCIPAL: pdfplumber ===
            # pdfplumber ofrece extracci√≥n de texto m√°s precisa y manejo de tablas
            import pdfplumber
            
            with pdfplumber.open(pdf_path) as pdf:
                logger.info(f"üìë Extrayendo texto de {len(pdf.pages)} p√°ginas")
                
                # Procesar cada p√°gina del PDF
                for page_num, page in enumerate(pdf.pages, 1):
                    try:
                        # Extraer texto de la p√°gina actual
                        page_text = page.extract_text()
                        
                        if page_text:
                            # Limpiar y procesar el texto extra√≠do
                            cleaned_text = self._clean_extracted_text(page_text)
                            extracted_text += f"\n{cleaned_text}\n"
                            
                        if page_num % 10 == 0:  # Log cada 10 p√°ginas
                            logger.info(f"   üìÑ Procesadas {page_num}/{len(pdf.pages)} p√°ginas")
                            
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error en p√°gina {page_num}: {e}")
                        continue  # Continuar con la siguiente p√°gina
        
        except ImportError:
            # Si pdfplumber no est√° disponible, usar PyPDF2 como fallback
            logger.warning("‚ö†Ô∏è pdfplumber no disponible, usando PyPDF2")
            extracted_text = self._extract_with_pypdf2(pdf_path)
            
        except Exception as e:
            # Si pdfplumber falla, intentar con PyPDF2
            logger.warning(f"‚ö†Ô∏è Error con pdfplumber: {e}, intentando PyPDF2")
            extracted_text = self._extract_with_pypdf2(pdf_path)
        
        # Verificar que se extrajo texto v√°lido
        if not extracted_text or len(extracted_text.strip()) < 100:
            logger.warning(f"‚ö†Ô∏è Texto extra√≠do insuficiente de {pdf_path.name}")
            return
        
        # Guardar el texto extra√≠do en archivo TXT
        try:
            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write(extracted_text)
            
            self.stats['texts_created'] += 1
            logger.info(f"üíæ Texto guardado: {txt_filename} ({len(extracted_text)} caracteres)")
            
        except Exception as e:
            error_msg = f"‚ùå Error guardando {txt_filename}: {e}"
            logger.error(error_msg)
            self.stats['errors'].append(error_msg)

    def _clean_extracted_text(self, text: str) -> str:
        """
        Limpia y normaliza el texto extra√≠do de PDFs
        
        Esta funci√≥n aplica una serie de transformaciones para mejorar
        la calidad del texto antes del procesamiento de embeddings.
        
        Args:
            text: Texto crudo extra√≠do del PDF
            
        Returns:
            str: Texto limpio y normalizado
        """
        if not text:
            return ""
        
        # === LIMPIEZA DE CARACTERES ===
        # Normalizar espacios en blanco
        text = re.sub(r'\s+', ' ', text)  # M√∫ltiples espacios ‚Üí un espacio
        text = re.sub(r'\n\s*\n', '\n\n', text)  # M√∫ltiples saltos ‚Üí doble salto
        
        # Eliminar caracteres de control y caracteres especiales problem√°ticos
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x84\x86-\x9f]', '', text)
        
        # === NORMALIZACI√ìN DE TEXTO ACAD√âMICO ===
        # Preservar referencias acad√©micas importantes
        # Patr√≥n para referencias como (Autor, 2020) o (Autor et al., 2020)
        text = re.sub(r'\(\s*([A-Z][a-zA-Z]+(?:\s+et\s+al\.)?),?\s*(\d{4})\s*\)', 
                     r'(\1, \2)', text)
        
        # Normalizar puntuaci√≥n
        text = re.sub(r'([.!?])\s*([A-Z])', r'\1 \2', text)  # Espacios despu√©s de puntos
        text = re.sub(r'\s*([,;:])\s*', r'\1 ', text)  # Espacios alrededor de puntuaci√≥n
        
        # === LIMPIEZA ESPEC√çFICA DE PDFs ===
        # Eliminar headers/footers comunes
        text = re.sub(r'^p√°gina\s+\d+.*?$', '', text, flags=re.MULTILINE | re.IGNORECASE)
        text = re.sub(r'^\d+\s*$', '', text, flags=re.MULTILINE)  # N√∫meros de p√°gina solos
        
        # Limpiar espacios finales
        text = text.strip()
        
        return text

    def _extract_with_pypdf2(self, pdf_path: Path) -> str:
        """
        M√©todo de fallback para extracci√≥n usando PyPDF2
        
        Se utiliza cuando pdfplumber no est√° disponible o falla.
        PyPDF2 es m√°s b√°sico pero m√°s compatible.
        
        Args:
            pdf_path: Ruta al archivo PDF
            
        Returns:
            str: Texto extra√≠do con PyPDF2
        """
        try:
            import PyPDF2
            
            extracted_text = ""
            
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                logger.info(f"üìë Extrayendo con PyPDF2: {len(pdf_reader.pages)} p√°ginas")
                
                # Procesar cada p√°gina
                for page_num, page in enumerate(pdf_reader.pages):
                    try:
                        page_text = page.extract_text()
                        if page_text:
                            cleaned_text = self._clean_extracted_text(page_text)
                            extracted_text += f"\n{cleaned_text}\n"
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error PyPDF2 p√°gina {page_num}: {e}")
                        continue
            
            return extracted_text
            
        except ImportError:
            logger.error("‚ùå PyPDF2 no disponible")
            return ""
        except Exception as e:
            logger.error(f"‚ùå Error con PyPDF2: {e}")
            return ""

    def smart_chunk_text(self, force_rechunk: bool = False) -> List[Dict]:
        """
        FASE TRANSFORM: Fragmenta texto en chunks inteligentes
        
        Esta funci√≥n implementa un algoritmo de chunking que:
        1. Respeta la estructura del documento (p√°rrafos, secciones)
        2. Preserva referencias acad√©micas
        3. Mantiene contexto con overlap entre chunks
        4. Optimiza para embeddings de 768 dimensiones
        
        Args:
            force_rechunk: Si True, reprocessa todos los textos
            
        Returns:
            List[Dict]: Lista de chunks con metadata
        """
        logger.info("üîÑ FASE TRANSFORM: Iniciando chunking inteligente")
        
        # Buscar todos los archivos TXT
        txt_files = list(self.texts_dir.glob("*.txt"))
        
        if not txt_files:
            logger.warning(f"‚ö†Ô∏è No se encontraron archivos TXT en {self.texts_dir}")
            return []
        
        logger.info(f"üìù Encontrados {len(txt_files)} archivos TXT para chunking")
        
        all_chunks = []
        
        # Procesar cada archivo TXT
        for txt_file in txt_files:
            try:
                chunks = self._chunk_single_file(txt_file, force_rechunk)
                all_chunks.extend(chunks)
                
            except Exception as e:
                error_msg = f"‚ùå Error chunking {txt_file.name}: {e}"
                logger.error(error_msg)
                self.stats['errors'].append(error_msg)
                continue
        
        self.stats['chunks_generated'] = len(all_chunks)
        logger.info(f"‚úÖ Chunking completado: {len(all_chunks)} chunks generados")
        
        return all_chunks

    def _chunk_single_file(self, txt_path: Path, force_rechunk: bool) -> List[Dict]:
        """
        Fragmenta un √∫nico archivo de texto en chunks inteligentes
        
        Args:
            txt_path: Ruta al archivo TXT
            force_rechunk: Si True, regenera chunks aunque ya existan
            
        Returns:
            List[Dict]: Lista de chunks con metadata para este archivo
        """
        logger.info(f"‚úÇÔ∏è Chunking: {txt_path.name}")
        
        # Leer el contenido del archivo
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            logger.error(f"‚ùå Error leyendo {txt_path.name}: {e}")
            return []
        
        if not content.strip():
            logger.warning(f"‚ö†Ô∏è Archivo vac√≠o: {txt_path.name}")
            return []
        
        # === ALGORITMO DE CHUNKING INTELIGENTE ===
        
        # 1. Dividir en p√°rrafos (preservar estructura)
        paragraphs = self._split_into_paragraphs(content)
        
        # 2. Agrupar p√°rrafos en chunks respetando l√≠mites
        chunks = self._group_paragraphs_into_chunks(paragraphs)
        
        # 3. Aplicar overlap entre chunks (preservar contexto)
        chunks_with_overlap = self._apply_chunk_overlap(chunks)
        
        # 4. Crear metadata para cada chunk
        chunks_with_metadata = []
        for i, chunk_text in enumerate(chunks_with_overlap):
            # Crear identificador √∫nico para el chunk
            chunk_id = f"{txt_path.stem}_chunk_{i:03d}"
            
            # Metadata completa para trazabilidad
            metadata = {
                'chunk_id': chunk_id,           # ID √∫nico del chunk
                'source_file': txt_path.name,   # Archivo fuente
                'chunk_index': i,               # √çndice dentro del archivo
                'total_chunks': len(chunks_with_overlap),  # Total de chunks del archivo
                'char_count': len(chunk_text),  # N√∫mero de caracteres
                'word_count': len(chunk_text.split()),  # N√∫mero aproximado de palabras
                'has_references': self._detect_academic_references(chunk_text),  # Referencias acad√©micas
                'file_stem': txt_path.stem      # Nombre del archivo sin extensi√≥n
            }
            
            chunks_with_metadata.append({
                'text': chunk_text,
                'metadata': metadata
            })
        
        logger.info(f"‚úÇÔ∏è {txt_path.name}: {len(chunks_with_metadata)} chunks creados")
        return chunks_with_metadata

    def _split_into_paragraphs(self, content: str) -> List[str]:
        """
        Divide el contenido en p√°rrafos respetando la estructura del documento
        
        Args:
            content: Texto completo del documento
            
        Returns:
            List[str]: Lista de p√°rrafos
        """
        # Dividir por dobles saltos de l√≠nea (p√°rrafos)
        paragraphs = re.split(r'\n\s*\n', content)
        
        # Limpiar y filtrar p√°rrafos
        cleaned_paragraphs = []
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            
            # Filtrar p√°rrafos muy cortos (probablemente ruido)
            if len(paragraph) > 50:  # M√≠nimo 50 caracteres
                cleaned_paragraphs.append(paragraph)
        
        return cleaned_paragraphs

    def _group_paragraphs_into_chunks(self, paragraphs: List[str]) -> List[str]:
        """
        Agrupa p√°rrafos en chunks respetando el l√≠mite de tama√±o
        
        Args:
            paragraphs: Lista de p√°rrafos
            
        Returns:
            List[str]: Lista de chunks
        """
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            # Verificar si agregar este p√°rrafo exceder√≠a el l√≠mite
            potential_chunk = current_chunk + "\n\n" + paragraph if current_chunk else paragraph
            
            if len(potential_chunk) <= self.chunk_size:
                # El p√°rrafo cabe en el chunk actual
                current_chunk = potential_chunk
            else:
                # El p√°rrafo no cabe, finalizar chunk actual
                if current_chunk:
                    chunks.append(current_chunk)
                
                # Iniciar nuevo chunk
                if len(paragraph) <= self.chunk_size:
                    current_chunk = paragraph
                else:
                    # P√°rrafo muy largo, dividir en sub-chunks
                    sub_chunks = self._split_long_paragraph(paragraph)
                    chunks.extend(sub_chunks[:-1])  # Agregar todos excepto el √∫ltimo
                    current_chunk = sub_chunks[-1]  # El √∫ltimo se vuelve el chunk actual
        
        # Agregar el √∫ltimo chunk si no est√° vac√≠o
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks

    def _split_long_paragraph(self, paragraph: str) -> List[str]:
        """
        Divide un p√°rrafo muy largo en sub-chunks m√°s peque√±os
        
        Args:
            paragraph: P√°rrafo que excede el tama√±o m√°ximo
            
        Returns:
            List[str]: Lista de sub-chunks
        """
        # Intentar dividir por oraciones
        sentences = re.split(r'(?<=[.!?])\s+', paragraph)
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            potential_chunk = current_chunk + " " + sentence if current_chunk else sentence
            
            if len(potential_chunk) <= self.chunk_size:
                current_chunk = potential_chunk
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                
                # Si la oraci√≥n sola es muy larga, dividir por palabras
                if len(sentence) > self.chunk_size:
                    word_chunks = self._split_by_words(sentence)
                    chunks.extend(word_chunks[:-1])
                    current_chunk = word_chunks[-1]
                else:
                    current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks

    def _split_by_words(self, text: str) -> List[str]:
        """
        Divide texto por palabras cuando las oraciones son muy largas
        
        Args:
            text: Texto a dividir
            
        Returns:
            List[str]: Lista de chunks por palabras
        """
        words = text.split()
        chunks = []
        current_chunk = ""
        
        for word in words:
            potential_chunk = current_chunk + " " + word if current_chunk else word
            
            if len(potential_chunk) <= self.chunk_size:
                current_chunk = potential_chunk
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = word
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks

    def _apply_chunk_overlap(self, chunks: List[str]) -> List[str]:
        """
        Aplica overlap entre chunks para preservar contexto
        
        El overlap ayuda a mantener continuidad sem√°ntica entre fragmentos,
        especialmente importante para referencias que cruzan l√≠mites de chunks.
        
        Args:
            chunks: Lista de chunks sin overlap
            
        Returns:
            List[str]: Lista de chunks con overlap aplicado
        """
        if len(chunks) <= 1:
            return chunks
        
        overlapped_chunks = [chunks[0]]  # El primer chunk no necesita overlap previo
        
        for i in range(1, len(chunks)):
            current_chunk = chunks[i]
            previous_chunk = chunks[i-1]
            
            # Extraer las √∫ltimas palabras del chunk anterior para overlap
            overlap_text = self._extract_overlap_text(previous_chunk, self.chunk_overlap)
            
            # Combinar overlap con chunk actual
            if overlap_text:
                overlapped_chunk = overlap_text + "\n\n" + current_chunk
            else:
                overlapped_chunk = current_chunk
            
            overlapped_chunks.append(overlapped_chunk)
        
        return overlapped_chunks

    def _extract_overlap_text(self, chunk: str, overlap_size: int) -> str:
        """
        Extrae texto de overlap del final de un chunk
        
        Args:
            chunk: Chunk del cual extraer overlap
            overlap_size: Tama√±o del overlap en caracteres
            
        Returns:
            str: Texto de overlap
        """
        if len(chunk) <= overlap_size:
            return chunk
        
        # Extraer √∫ltimos caracteres
        overlap_text = chunk[-overlap_size:]
        
        # Intentar cortar en l√≠mite de palabra para evitar cortes bruscos
        space_index = overlap_text.find(' ')
        if space_index > 0:
            overlap_text = overlap_text[space_index:].strip()
        
        return overlap_text

    def _detect_academic_references(self, text: str) -> bool:
        """
        Detecta si el texto contiene referencias acad√©micas
        
        Args:
            text: Texto a analizar
            
        Returns:
            bool: True si contiene referencias acad√©micas
        """
        # Patrones de referencias acad√©micas comunes
        patterns = [
            r'\([A-Z][a-zA-Z]+,?\s+\d{4}\)',  # (Autor, 2020)
            r'\([A-Z][a-zA-Z]+\s+et\s+al\.,?\s+\d{4}\)',  # (Autor et al., 2020)
            r'\d{4}[\):]',  # A√±os seguidos de ) o :
            r'[Ss]eg√∫n\s+[A-Z][a-zA-Z]+',  # Seg√∫n Autor
            r'[Cc]omo\s+se√±ala\s+[A-Z][a-zA-Z]+',  # Como se√±ala Autor
        ]
        
        for pattern in patterns:
            if re.search(pattern, text):
                return True
        
        return False

    def generate_embeddings_and_load(self, chunks: List[Dict]) -> bool:
        """
        FASE LOAD: Genera embeddings y carga en ChromaDB
        
        Esta es la fase final del ETL que:
        1. Inicializa el modelo de embeddings (all-mpnet-base-v2)
        2. Genera vectores de 768 dimensiones para cada chunk
        3. Almacena vectores y metadata en ChromaDB
        4. Crea √≠ndices para b√∫squeda eficiente
        
        Args:
            chunks: Lista de chunks con metadata
            
        Returns:
            bool: True si la carga fue exitosa
        """
        logger.info("üîÑ FASE LOAD: Generando embeddings y cargando en ChromaDB")
        
        if not chunks:
            logger.warning("‚ö†Ô∏è No hay chunks para procesar")
            return False
        
        try:
            # === INICIALIZACI√ìN DE COMPONENTES ===
            
            # 1. Importar bibliotecas necesarias
            from sentence_transformers import SentenceTransformer
            import chromadb
            
            # 2. Inicializar modelo de embeddings
            logger.info(f"ü§ñ Cargando modelo: {self.embedding_model}")
            model = SentenceTransformer(self.embedding_model)
            
            # Informaci√≥n del modelo cargado
            logger.info(f"üìä Dimensiones del modelo: {model.get_sentence_embedding_dimension()}")
            
            # 3. Inicializar cliente ChromaDB
            logger.info(f"üóÑÔ∏è Inicializando ChromaDB en: {self.chroma_dir}")
            client = chromadb.PersistentClient(path=str(self.chroma_dir))
            
            # 4. Crear o obtener colecci√≥n
            collection_name = "simple_rag_docs"
            
            # Eliminar colecci√≥n existente si se est√° forzando recreaci√≥n
            try:
                existing_collection = client.get_collection(collection_name)
                logger.info("üóëÔ∏è Eliminando colecci√≥n existente para recrear")
                client.delete_collection(collection_name)
            except:
                pass  # La colecci√≥n no existe, continuar
            
            # Crear nueva colecci√≥n
            collection = client.create_collection(
                name=collection_name,
                metadata={"description": "RAG documents with academic embeddings"}
            )
            
            logger.info(f"‚úÖ Colecci√≥n '{collection_name}' creada")
            
            # === PROCESAMIENTO EN LOTES ===
            
            batch_size = 50  # Procesar en lotes para eficiencia de memoria
            total_chunks = len(chunks)
            
            logger.info(f"üì¶ Procesando {total_chunks} chunks en lotes de {batch_size}")
            
            for i in range(0, total_chunks, batch_size):
                batch = chunks[i:i + batch_size]
                batch_num = (i // batch_size) + 1
                total_batches = (total_chunks + batch_size - 1) // batch_size
                
                logger.info(f"üîÑ Procesando lote {batch_num}/{total_batches} ({len(batch)} chunks)")
                
                # Procesar lote actual
                success = self._process_embedding_batch(model, collection, batch, i)
                
                if not success:
                    logger.error(f"‚ùå Error en lote {batch_num}")
                    continue
                
                # Actualizar estad√≠sticas
                self.stats['embeddings_created'] += len(batch)
                
                # Log de progreso
                progress = (i + len(batch)) / total_chunks * 100
                logger.info(f"üìà Progreso: {progress:.1f}% ({i + len(batch)}/{total_chunks})")
            
            # === VERIFICACI√ìN FINAL ===
            
            # Verificar que la colecci√≥n se pobl√≥ correctamente
            final_count = collection.count()
            logger.info(f"üéØ Verificaci√≥n final: {final_count} documentos en ChromaDB")
            
            if final_count != total_chunks:
                logger.warning(f"‚ö†Ô∏è Discrepancia: esperados {total_chunks}, guardados {final_count}")
            
            logger.info(f"‚úÖ LOAD completado: {final_count} embeddings generados y almacenados")
            return True
            
        except Exception as e:
            error_msg = f"‚ùå Error en generaci√≥n de embeddings: {e}"
            logger.error(error_msg)
            self.stats['errors'].append(error_msg)
            return False

    def _process_embedding_batch(self, model, collection, batch: List[Dict], start_index: int) -> bool:
        """
        Procesa un lote de chunks para generar embeddings
        
        Args:
            model: Modelo de SentenceTransformers cargado
            collection: Colecci√≥n de ChromaDB
            batch: Lote de chunks a procesar
            start_index: √çndice de inicio para IDs √∫nicos
            
        Returns:
            bool: True si el lote se proces√≥ exitosamente
        """
        try:
            # === PREPARACI√ìN DE DATOS ===
            
            # Extraer textos y metadatos del lote
            texts = [chunk['text'] for chunk in batch]
            metadatas = [chunk['metadata'] for chunk in batch]
            
            # Generar IDs √∫nicos para cada chunk
            ids = [f"doc_{start_index + i:06d}" for i in range(len(batch))]
            
            # === GENERACI√ìN DE EMBEDDINGS ===
            
            logger.info(f"üß† Generando embeddings para {len(texts)} textos...")
            
            # Generar embeddings usando el modelo
            # El modelo all-mpnet-base-v2 produce vectores de 768 dimensiones
            embeddings = model.encode(
                texts,
                batch_size=32,          # Lotes internos para eficiencia GPU/CPU
                show_progress_bar=False, # Evitar logs excesivos
                convert_to_numpy=True   # Convertir a numpy para ChromaDB
            )
            
            logger.info(f"üéØ Embeddings generados: {embeddings.shape}")
            
            # === ALMACENAMIENTO EN CHROMADB ===
            
            # Convertir embeddings a lista (requerido por ChromaDB)
            embeddings_list = embeddings.tolist()
            
            # Insertar en ChromaDB
            collection.add(
                embeddings=embeddings_list,  # Vectores de 768 dimensiones
                metadatas=metadatas,         # Metadata de cada chunk
                documents=texts,             # Texto original para referencia
                ids=ids                      # IDs √∫nicos
            )
            
            logger.info(f"üíæ Lote guardado en ChromaDB: {len(ids)} documentos")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error procesando lote: {e}")
            return False

    def run_complete_etl(self, force: bool = False):
        """
        Ejecuta el pipeline ETL completo
        
        Esta funci√≥n orquesta todo el proceso ETL:
        1. Crear directorios necesarios
        2. Extraer texto de PDFs (EXTRACT)
        3. Procesar y fragmentar texto (TRANSFORM)
        4. Generar embeddings y cargar en ChromaDB (LOAD)
        5. Generar reporte de estad√≠sticas
        
        Args:
            force: Si True, regenera todo desde cero
        """
        start_time = time.time()
        
        logger.info("üöÄ INICIANDO PROCESO ETL COMPLETO")
        logger.info("=" * 50)
        
        try:
            # === FASE 0: PREPARACI√ìN ===
            logger.info("üìÅ Fase 0: Preparaci√≥n de directorios")
            self.create_directories()
            
            # === FASE 1: EXTRACT ===
            logger.info("\nüìñ Fase 1: EXTRACT - Extracci√≥n de PDFs")
            extract_success = self.extract_from_pdfs()
            
            if not extract_success:
                logger.error("‚ùå Error en fase EXTRACT")
                return False
            
            # === FASE 2: TRANSFORM ===
            logger.info("\n‚úÇÔ∏è Fase 2: TRANSFORM - Chunking inteligente")
            chunks = self.smart_chunk_text(force_rechunk=force)
            
            if not chunks:
                logger.error("‚ùå Error en fase TRANSFORM")
                return False
            
            # === FASE 3: LOAD ===
            logger.info("\nüß† Fase 3: LOAD - Embeddings y ChromaDB")
            load_success = self.generate_embeddings_and_load(chunks)
            
            if not load_success:
                logger.error("‚ùå Error en fase LOAD")
                return False
            
            # === FINALIZACI√ìN ===
            end_time = time.time()
            self.stats['total_time'] = end_time - start_time
            
            # Generar reporte final
            self._generate_final_report()
            
            logger.info("üéâ PROCESO ETL COMPLETADO EXITOSAMENTE")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error cr√≠tico en ETL: {e}")
            self.stats['errors'].append(f"Error cr√≠tico: {e}")
            return False

    def _generate_final_report(self):
        """
        Genera reporte final con estad√≠sticas del proceso ETL
        """
        logger.info("\nüìä REPORTE FINAL DEL PROCESO ETL")
        logger.info("=" * 40)
        
        # Estad√≠sticas de archivos
        logger.info(f"üìÑ PDFs procesados: {self.stats['pdfs_processed']}")
        logger.info(f"üìù Archivos TXT creados: {self.stats['texts_created']}")
        logger.info(f"‚úÇÔ∏è Chunks generados: {self.stats['chunks_generated']}")
        logger.info(f"üß† Embeddings creados: {self.stats['embeddings_created']}")
        
        # Tiempo total
        total_minutes = self.stats['total_time'] / 60
        logger.info(f"‚è±Ô∏è Tiempo total: {total_minutes:.2f} minutos")
        
        # M√©tricas de rendimiento
        if self.stats['total_time'] > 0:
            chunks_per_second = self.stats['chunks_generated'] / self.stats['total_time']
            logger.info(f"üöÄ Velocidad: {chunks_per_second:.2f} chunks/segundo")
        
        # Errores encontrados
        if self.stats['errors']:
            logger.warning(f"‚ö†Ô∏è Errores encontrados: {len(self.stats['errors'])}")
            for error in self.stats['errors'][-5:]:  # Mostrar √∫ltimos 5 errores
                logger.warning(f"   - {error}")
        else:
            logger.info("‚úÖ Proceso completado sin errores")
        
        # Informaci√≥n de la base de datos
        try:
            import chromadb
            client = chromadb.PersistentClient(path=str(self.chroma_dir))
            collection = client.get_collection("simple_rag_docs")
            final_count = collection.count()
            logger.info(f"üóÑÔ∏è ChromaDB: {final_count} documentos almacenados")
        except:
            logger.warning("‚ö†Ô∏è No se pudo verificar ChromaDB")

def main():
    """
    Funci√≥n principal para ejecutar el ETL desde l√≠nea de comandos
    
    Acepta argumentos de l√≠nea de comandos para personalizar el procesamiento:
    - --force: Regenerar todo desde cero
    - --chunk-size: Tama√±o de chunks personalizado
    - --model: Modelo de embeddings alternativo
    """
    # === CONFIGURACI√ìN DE ARGUMENTOS ===
    parser = argparse.ArgumentParser(description='Procesador ETL completo para RAG Asistente')
    
    parser.add_argument('--force', action='store_true', 
                       help='Regenerar todo desde cero (eliminar datos existentes)')
    
    parser.add_argument('--chunk-size', type=int, default=800,
                       help='Tama√±o de chunks en caracteres (default: 800)')
    
    parser.add_argument('--chunk-overlap', type=int, default=100,
                       help='Solapamiento entre chunks (default: 100)')
    
    parser.add_argument('--model', type=str, default='sentence-transformers/all-mpnet-base-v2',
                       help='Modelo de embeddings a utilizar')
    
    args = parser.parse_args()
    
    # === INICIALIZACI√ìN DEL PROCESADOR ===
    logger.info("üîß Inicializando procesador ETL con configuraci√≥n:")
    logger.info(f"   - Force rebuild: {args.force}")
    logger.info(f"   - Chunk size: {args.chunk_size}")
    logger.info(f"   - Chunk overlap: {args.chunk_overlap}")
    logger.info(f"   - Modelo: {args.model}")
    
    # Crear procesador con configuraci√≥n personalizada
    processor = RAGETLProcessor()
    processor.chunk_size = args.chunk_size
    processor.chunk_overlap = args.chunk_overlap
    processor.embedding_model = args.model
    
    # === EJECUCI√ìN DEL PROCESO ===
    try:
        success = processor.run_complete_etl(force=args.force)
        
        if success:
            logger.info("üéâ ETL completado exitosamente")
            sys.exit(0)
        else:
            logger.error("‚ùå ETL fall√≥")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.warning("‚ö†Ô∏è Proceso interrumpido por usuario")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Error inesperado: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()