"""
üß∞ UTILIDADES RAG - FUNCIONES DE SOPORTE COMPLETAMENTE DOCUMENTADAS
==================================================================

Este archivo contiene todas las funciones auxiliares y utilidades
que soportan el sistema RAG con DeepSeek V3.

FUNCIONES PRINCIPALES:
1. Gesti√≥n de embeddings y chunking inteligente
2. Optimizaci√≥n de consultas y postprocesamiento
3. Manejo de errores y logging avanzado
4. Integraci√≥n con APIs externas
5. Cache y persistencia de resultados

TECNOLOG√çAS UTILIZADAS:
- sentence-transformers: Modelo all-mpnet-base-v2
- ChromaDB: Base de datos vectorial con HNSW
- tiktoken: Tokenizaci√≥n precisa para chunking
- asyncio: Operaciones as√≠ncronas
- threading: Procesamiento paralelo
"""

import os
import re
import json
import hashlib
import asyncio
import logging
from typing import List, Dict, Any, Optional, Union, Tuple
from pathlib import Path
from datetime import datetime
import time

# === CONFIGURACI√ìN DE LOGGING ===
logger = logging.getLogger(__name__)

# === CONFIGURACI√ìN DE CHUNKING ===
DEFAULT_CHUNK_SIZE = 800        # Tokens por chunk (√≥ptimo para embeddings)
DEFAULT_OVERLAP = 100           # Overlap entre chunks para contexto
MAX_CHUNK_SIZE = 1000          # M√°ximo tama√±o de chunk
MIN_CHUNK_SIZE = 200           # M√≠nimo tama√±o de chunk

class EmbeddingCache:
    """
    Sistema de cache para embeddings generados
    
    PROP√ìSITO:
    - Evitar regenerar embeddings de textos ya procesados
    - Acelerar respuestas para queries frecuentes
    - Reducir uso de recursos computacionales
    - Persistir embeddings entre sesiones
    
    IMPLEMENTACI√ìN:
    - Hash MD5 del texto como clave √∫nica
    - Almacenamiento en archivo JSON
    - Verificaci√≥n de integridad autom√°tica
    - Limpieza de cache obsoleto
    """
    
    def __init__(self, cache_dir: str = "cache"):
        """
        Inicializa el sistema de cache de embeddings
        
        Args:
            cache_dir (str): Directorio para almacenar el cache
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.cache_file = self.cache_dir / "embeddings_cache.json"
        self.cache_data = self._load_cache()
        
        logger.info(f"üìÅ Cache de embeddings inicializado: {self.cache_file}")
        logger.info(f"üìä Entradas en cache: {len(self.cache_data)}")
    
    def _load_cache(self) -> Dict[str, Any]:
        """
        Carga cache existente desde disco
        
        Returns:
            Dict: Datos del cache o diccionario vac√≠o si no existe
        """
        try:
            if self.cache_file.exists():
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    logger.debug(f"‚úÖ Cache cargado: {len(data)} entradas")
                    return data
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error cargando cache: {e}")
        
        return {}
    
    def _save_cache(self):
        """
        Guarda cache actual en disco
        """
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache_data, f, indent=2, ensure_ascii=False)
                logger.debug(f"üíæ Cache guardado: {len(self.cache_data)} entradas")
        except Exception as e:
            logger.error(f"‚ùå Error guardando cache: {e}")
    
    def get_hash(self, text: str) -> str:
        """
        Genera hash √∫nico para un texto
        
        Args:
            text (str): Texto a hashear
            
        Returns:
            str: Hash MD5 del texto
        """
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def get_embedding(self, text: str) -> Optional[List[float]]:
        """
        Obtiene embedding desde cache si existe
        
        Args:
            text (str): Texto para buscar embedding
            
        Returns:
            Optional[List[float]]: Embedding si existe en cache, None si no
        """
        text_hash = self.get_hash(text)
        
        if text_hash in self.cache_data:
            entry = self.cache_data[text_hash]
            logger.debug(f"üéØ Cache hit para: {text[:50]}...")
            return entry.get('embedding')
        
        logger.debug(f"‚ùå Cache miss para: {text[:50]}...")
        return None
    
    def store_embedding(self, text: str, embedding: List[float]):
        """
        Almacena embedding en cache
        
        Args:
            text (str): Texto original
            embedding (List[float]): Vector de embedding
        """
        text_hash = self.get_hash(text)
        
        self.cache_data[text_hash] = {
            'embedding': embedding,
            'text_preview': text[:100],  # Para debugging
            'timestamp': datetime.now().isoformat(),
            'dimensions': len(embedding)
        }
        
        self._save_cache()
        logger.debug(f"üíæ Embedding almacenado en cache: {text_hash}")
    
    def clean_cache(self, max_age_days: int = 30):
        """
        Limpia entradas antiguas del cache
        
        Args:
            max_age_days (int): M√°xima edad en d√≠as para mantener entradas
        """
        current_time = datetime.now()
        removed_count = 0
        
        keys_to_remove = []
        for key, entry in self.cache_data.items():
            try:
                entry_time = datetime.fromisoformat(entry.get('timestamp', ''))
                age_days = (current_time - entry_time).days
                
                if age_days > max_age_days:
                    keys_to_remove.append(key)
            except Exception:
                # Entradas sin timestamp v√°lido, remover
                keys_to_remove.append(key)
        
        for key in keys_to_remove:
            del self.cache_data[key]
            removed_count += 1
        
        if removed_count > 0:
            self._save_cache()
            logger.info(f"üßπ Cache limpiado: {removed_count} entradas removidas")

class SmartChunker:
    """
    Sistema avanzado de chunking para optimizar embeddings
    
    CARACTER√çSTICAS:
    - Chunking sem√°ntico preservando contexto
    - Overlap inteligente para mantener coherencia
    - Respeto por l√≠mites de oraciones y p√°rrafos
    - Optimizaci√≥n para modelo all-mpnet-base-v2
    - Manejo especial de referencias y citas acad√©micas
    
    ALGORITMO:
    1. An√°lisis de estructura del texto (p√°rrafos, oraciones)
    2. Tokenizaci√≥n precisa con tiktoken
    3. Divisi√≥n sem√°ntica preservando contexto
    4. Overlap calculado para m√°xima coherencia
    5. Validaci√≥n de calidad de chunks
    """
    
    def __init__(self, chunk_size: int = DEFAULT_CHUNK_SIZE, overlap: int = DEFAULT_OVERLAP):
        """
        Inicializa el sistema de chunking inteligente
        
        Args:
            chunk_size (int): Tama√±o objetivo en tokens
            overlap (int): Overlap entre chunks en tokens
        """
        self.chunk_size = chunk_size
        self.overlap = overlap
        
        # Inicializar tokenizador
        try:
            import tiktoken
            # Usar encoding de GPT-3.5 que es compatible con embeddings
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
            logger.info("üî§ Tokenizador tiktoken inicializado")
        except ImportError:
            logger.warning("‚ö†Ô∏è tiktoken no disponible, usando aproximaci√≥n")
            self.tokenizer = None
        
        logger.info(f"‚úÇÔ∏è Chunker configurado: {chunk_size} tokens, overlap {overlap}")
    
    def count_tokens(self, text: str) -> int:
        """
        Cuenta tokens en el texto de forma precisa
        
        Args:
            text (str): Texto a contar
            
        Returns:
            int: N√∫mero de tokens
        """
        if self.tokenizer:
            return len(self.tokenizer.encode(text))
        else:
            # Aproximaci√≥n: promedio de 4 caracteres por token
            return len(text) // 4
    
    def split_into_sentences(self, text: str) -> List[str]:
        """
        Divide texto en oraciones respetando contexto acad√©mico
        
        CARACTER√çSTICAS:
        - Respeta abreviaciones acad√©micas (et al., etc., Dr.)
        - Mantiene citas y referencias intactas
        - Preserva numeraci√≥n y listas
        - Maneja puntuaci√≥n compleja
        
        Args:
            text (str): Texto a dividir
            
        Returns:
            List[str]: Lista de oraciones
        """
        # Proteger abreviaciones comunes
        text = text.replace('et al.', 'et al¬ß')
        text = text.replace('etc.', 'etc¬ß')
        text = text.replace('Dr.', 'Dr¬ß')
        text = text.replace('Prof.', 'Prof¬ß')
        text = text.replace('p.', 'p¬ß')
        text = text.replace('pp.', 'pp¬ß')
        
        # Divisi√≥n b√°sica por puntos
        sentences = re.split(r'\.(?=\s+[A-Z])', text)
        
        # Restaurar abreviaciones
        sentences = [s.replace('¬ß', '.') for s in sentences]
        
        # Limpiar y filtrar oraciones v√°lidas
        valid_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 10:  # Filtrar fragmentos muy cortos
                valid_sentences.append(sentence)
        
        return valid_sentences
    
    def split_into_paragraphs(self, text: str) -> List[str]:
        """
        Divide texto en p√°rrafos manteniendo estructura
        
        Args:
            text (str): Texto a dividir
            
        Returns:
            List[str]: Lista de p√°rrafos
        """
        # Divisi√≥n por doble salto de l√≠nea
        paragraphs = re.split(r'\n\s*\n', text)
        
        # Limpiar y filtrar p√°rrafos v√°lidos
        valid_paragraphs = []
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if len(paragraph) > 50:  # Filtrar p√°rrafos muy cortos
                valid_paragraphs.append(paragraph)
        
        return valid_paragraphs
    
    def smart_chunk(self, text: str) -> List[Dict[str, Any]]:
        """
        Realiza chunking inteligente del texto
        
        ALGORITMO DETALLADO:
        1. AN√ÅLISIS INICIAL: Eval√∫a longitud y estructura
        2. DIVISI√ìN SEM√ÅNTICA: Por p√°rrafos o oraciones
        3. OPTIMIZACI√ìN: Ajusta tama√±os para modelo de embeddings
        4. OVERLAP: Calcula overlap √≥ptimo para coherencia
        5. VALIDACI√ìN: Verifica calidad de chunks generados
        
        Args:
            text (str): Texto a procesar
            
        Returns:
            List[Dict]: Lista de chunks con metadata
        """
        if not text or len(text.strip()) < 10:
            logger.warning("‚ö†Ô∏è Texto muy corto para chunking")
            return []
        
        # === FASE 1: AN√ÅLISIS INICIAL ===
        total_tokens = self.count_tokens(text)
        logger.debug(f"üìä Analizando texto: {total_tokens} tokens")
        
        # Si el texto es peque√±o, retornar como un solo chunk
        if total_tokens <= self.chunk_size:
            logger.debug("üìÑ Texto peque√±o, chunk √∫nico")
            return [{
                'text': text.strip(),
                'tokens': total_tokens,
                'chunk_id': 0,
                'overlap_start': 0,
                'overlap_end': 0
            }]
        
        # === FASE 2: ESTRATEGIA DE DIVISI√ìN ===
        paragraphs = self.split_into_paragraphs(text)
        
        if len(paragraphs) > 1:
            logger.debug(f"üìù Divisi√≥n por p√°rrafos: {len(paragraphs)} p√°rrafos")
            return self._chunk_by_paragraphs(paragraphs)
        else:
            logger.debug("üìù Divisi√≥n por oraciones")
            sentences = self.split_into_sentences(text)
            return self._chunk_by_sentences(sentences)
    
    def _chunk_by_paragraphs(self, paragraphs: List[str]) -> List[Dict[str, Any]]:
        """
        Crea chunks bas√°ndose en p√°rrafos
        
        Args:
            paragraphs (List[str]): Lista de p√°rrafos
            
        Returns:
            List[Dict]: Chunks generados
        """
        chunks = []
        current_chunk = ""
        current_tokens = 0
        chunk_id = 0
        
        for paragraph in paragraphs:
            paragraph_tokens = self.count_tokens(paragraph)
            
            # Si el p√°rrafo es muy grande, dividirlo por oraciones
            if paragraph_tokens > self.chunk_size:
                # Guardar chunk actual si existe
                if current_chunk:
                    chunks.append(self._create_chunk_dict(current_chunk, chunk_id))
                    chunk_id += 1
                    current_chunk = ""
                    current_tokens = 0
                
                # Procesar p√°rrafo grande por oraciones
                sentences = self.split_into_sentences(paragraph)
                sentence_chunks = self._chunk_by_sentences(sentences, start_id=chunk_id)
                chunks.extend(sentence_chunks)
                chunk_id += len(sentence_chunks)
            
            # Si agregar el p√°rrafo excede el l√≠mite
            elif current_tokens + paragraph_tokens > self.chunk_size:
                # Guardar chunk actual
                if current_chunk:
                    chunks.append(self._create_chunk_dict(current_chunk, chunk_id))
                    chunk_id += 1
                
                # Calcular overlap con p√°rrafo anterior
                current_chunk = self._calculate_overlap(current_chunk, paragraph)
                current_tokens = self.count_tokens(current_chunk)
            
            else:
                # Agregar p√°rrafo al chunk actual
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph
                current_tokens += paragraph_tokens
        
        # Agregar √∫ltimo chunk si existe
        if current_chunk:
            chunks.append(self._create_chunk_dict(current_chunk, chunk_id))
        
        return self._add_overlap_metadata(chunks)
    
    def _chunk_by_sentences(self, sentences: List[str], start_id: int = 0) -> List[Dict[str, Any]]:
        """
        Crea chunks bas√°ndose en oraciones
        
        Args:
            sentences (List[str]): Lista de oraciones
            start_id (int): ID inicial para chunks
            
        Returns:
            List[Dict]: Chunks generados
        """
        chunks = []
        current_chunk = ""
        current_tokens = 0
        chunk_id = start_id
        
        for sentence in sentences:
            sentence_tokens = self.count_tokens(sentence)
            
            # Si una oraci√≥n es muy larga, dividirla forzadamente
            if sentence_tokens > self.chunk_size:
                # Guardar chunk actual si existe
                if current_chunk:
                    chunks.append(self._create_chunk_dict(current_chunk, chunk_id))
                    chunk_id += 1
                
                # Dividir oraci√≥n larga
                forced_chunks = self._force_split_text(sentence, chunk_id)
                chunks.extend(forced_chunks)
                chunk_id += len(forced_chunks)
                current_chunk = ""
                current_tokens = 0
                continue
            
            # Si agregar la oraci√≥n excede el l√≠mite
            if current_tokens + sentence_tokens > self.chunk_size:
                # Guardar chunk actual
                if current_chunk:
                    chunks.append(self._create_chunk_dict(current_chunk, chunk_id))
                    chunk_id += 1
                
                # Calcular overlap
                current_chunk = self._calculate_overlap(current_chunk, sentence)
                current_tokens = self.count_tokens(current_chunk)
            
            else:
                # Agregar oraci√≥n al chunk actual
                if current_chunk:
                    current_chunk += " " + sentence
                else:
                    current_chunk = sentence
                current_tokens += sentence_tokens
        
        # Agregar √∫ltimo chunk si existe
        if current_chunk:
            chunks.append(self._create_chunk_dict(current_chunk, chunk_id))
        
        return self._add_overlap_metadata(chunks)
    
    def _force_split_text(self, text: str, start_id: int) -> List[Dict[str, Any]]:
        """
        Divisi√≥n forzada de texto muy largo
        
        Args:
            text (str): Texto a dividir
            start_id (int): ID inicial
            
        Returns:
            List[Dict]: Chunks forzados
        """
        chunks = []
        words = text.split()
        current_chunk = ""
        current_tokens = 0
        chunk_id = start_id
        
        for word in words:
            word_tokens = self.count_tokens(word)
            
            if current_tokens + word_tokens > self.chunk_size:
                if current_chunk:
                    chunks.append(self._create_chunk_dict(current_chunk, chunk_id))
                    chunk_id += 1
                
                current_chunk = word
                current_tokens = word_tokens
            else:
                if current_chunk:
                    current_chunk += " " + word
                else:
                    current_chunk = word
                current_tokens += word_tokens
        
        if current_chunk:
            chunks.append(self._create_chunk_dict(current_chunk, chunk_id))
        
        return chunks
    
    def _calculate_overlap(self, previous_chunk: str, next_text: str) -> str:
        """
        Calcula overlap √≥ptimo entre chunks
        
        Args:
            previous_chunk (str): Chunk anterior
            next_text (str): Texto siguiente
            
        Returns:
            str: Texto con overlap calculado
        """
        if not previous_chunk or self.overlap <= 0:
            return next_text
        
        # Obtener √∫ltimas oraciones del chunk anterior para overlap
        previous_sentences = self.split_into_sentences(previous_chunk)
        overlap_text = ""
        overlap_tokens = 0
        
        # Agregar oraciones desde el final hasta alcanzar overlap deseado
        for sentence in reversed(previous_sentences):
            sentence_tokens = self.count_tokens(sentence)
            if overlap_tokens + sentence_tokens <= self.overlap:
                overlap_text = sentence + " " + overlap_text if overlap_text else sentence
                overlap_tokens += sentence_tokens
            else:
                break
        
        # Combinar overlap con nuevo texto
        if overlap_text:
            return overlap_text.strip() + " " + next_text
        else:
            return next_text
    
    def _create_chunk_dict(self, text: str, chunk_id: int) -> Dict[str, Any]:
        """
        Crea diccionario de chunk con metadata
        
        Args:
            text (str): Texto del chunk
            chunk_id (int): ID del chunk
            
        Returns:
            Dict: Chunk con metadata
        """
        return {
            'text': text.strip(),
            'tokens': self.count_tokens(text),
            'chunk_id': chunk_id,
            'length': len(text),
            'preview': text[:100] + "..." if len(text) > 100 else text
        }
    
    def _add_overlap_metadata(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Agrega metadata de overlap a los chunks
        
        Args:
            chunks (List[Dict]): Lista de chunks
            
        Returns:
            List[Dict]: Chunks con metadata de overlap
        """
        for i, chunk in enumerate(chunks):
            chunk['overlap_start'] = i > 0
            chunk['overlap_end'] = i < len(chunks) - 1
            chunk['total_chunks'] = len(chunks)
            chunk['position'] = i
        
        return chunks

class QueryOptimizer:
    """
    Optimizador de queries para mejorar resultados de b√∫squeda
    
    FUNCIONALIDADES:
    - Expansi√≥n de t√©rminos con sin√≥nimos
    - Correcci√≥n ortogr√°fica autom√°tica
    - Normalizaci√≥n de t√©rminos acad√©micos
    - Detecci√≥n de entidades (nombres, fechas, conceptos)
    - Optimizaci√≥n espec√≠fica para corpus acad√©mico
    """
    
    def __init__(self):
        """
        Inicializa el optimizador de queries
        """
        # Diccionario de sin√≥nimos acad√©micos
        self.synonyms = {
            'estado': ['gobierno', 'naci√≥n', 'pa√≠s', 'rep√∫blica'],
            'sociedad': ['comunidad', 'pueblo', 'ciudadan√≠a', 'poblaci√≥n'],
            'pol√≠tica': ['pol√≠tico', 'gubernamental', 'estatal', 'p√∫blico'],
            'democracia': ['democr√°tico', 'participaci√≥n', 'electoral'],
            'desarrollo': ['crecimiento', 'progreso', 'evoluci√≥n', 'avance'],
            'econ√≥mico': ['econom√≠a', 'financiero', 'monetario', 'fiscal'],
            'social': ['sociedad', 'comunitario', 'colectivo', 'p√∫blico'],
            'cultural': ['cultura', 'tradici√≥n', 'identidad', 'patrimonio']
        }
        
        # T√©rminos acad√©micos comunes
        self.academic_terms = {
            'an√°lisis', 'estudio', 'investigaci√≥n', 'metodolog√≠a',
            'teor√≠a', 'concepto', 'enfoque', 'perspectiva',
            'marco', 'modelo', 'paradigma', 'sistema'
        }
        
        logger.info("üîç Optimizador de queries inicializado")
    
    def optimize_query(self, query: str) -> Dict[str, Any]:
        """
        Optimiza una query para mejorar resultados de b√∫squeda
        
        Args:
            query (str): Query original del usuario
            
        Returns:
            Dict: Query optimizada con metadata
        """
        original_query = query
        
        # === FASE 1: NORMALIZACI√ìN ===
        normalized_query = self._normalize_query(query)
        
        # === FASE 2: EXPANSI√ìN CON SIN√ìNIMOS ===
        expanded_terms = self._expand_with_synonyms(normalized_query)
        
        # === FASE 3: DETECCI√ìN DE ENTIDADES ===
        entities = self._extract_entities(normalized_query)
        
        # === FASE 4: T√âRMINOS CLAVE ===
        key_terms = self._extract_key_terms(normalized_query)
        
        # === FASE 5: SUGERENCIAS DE MEJORA ===
        suggestions = self._generate_suggestions(normalized_query)
        
        return {
            'original_query': original_query,
            'normalized_query': normalized_query,
            'expanded_terms': expanded_terms,
            'entities': entities,
            'key_terms': key_terms,
            'suggestions': suggestions,
            'academic_score': self._calculate_academic_score(normalized_query)
        }
    
    def _normalize_query(self, query: str) -> str:
        """
        Normaliza la query eliminando ruido y estandarizando formato
        
        Args:
            query (str): Query original
            
        Returns:
            str: Query normalizada
        """
        # Convertir a min√∫sculas
        normalized = query.lower().strip()
        
        # Eliminar caracteres especiales innecesarios
        normalized = re.sub(r'[^\w\s\-\.,;:()]', ' ', normalized)
        
        # Normalizar espacios
        normalized = re.sub(r'\s+', ' ', normalized)
        
        # Eliminar palabras muy comunes al inicio
        start_words = ['c√≥mo', 'qu√©', 'cu√°l', 'cu√°les', 'd√≥nde', 'cu√°ndo', 'por qu√©']
        for word in start_words:
            if normalized.startswith(word + ' '):
                normalized = normalized[len(word):].strip()
                break
        
        return normalized
    
    def _expand_with_synonyms(self, query: str) -> List[str]:
        """
        Expande query con sin√≥nimos relevantes
        
        Args:
            query (str): Query normalizada
            
        Returns:
            List[str]: Lista de t√©rminos expandidos
        """
        words = query.split()
        expanded_terms = []
        
        for word in words:
            expanded_terms.append(word)
            
            # Buscar sin√≥nimos
            for term, synonyms in self.synonyms.items():
                if word == term or word in synonyms:
                    expanded_terms.extend([s for s in synonyms if s not in expanded_terms])
        
        return expanded_terms
    
    def _extract_entities(self, query: str) -> Dict[str, List[str]]:
        """
        Extrae entidades de la query
        
        Args:
            query (str): Query a analizar
            
        Returns:
            Dict: Entidades por categor√≠a
        """
        entities = {
            'persons': [],
            'years': [],
            'places': [],
            'concepts': []
        }
        
        # Detectar a√±os
        years = re.findall(r'\b(19|20)\d{2}\b', query)
        entities['years'] = years
        
        # Detectar nombres propios (aproximaci√≥n)
        proper_nouns = re.findall(r'\b[A-Z][a-z]+\b', query)
        entities['persons'] = proper_nouns
        
        # Detectar t√©rminos acad√©micos
        for term in self.academic_terms:
            if term in query:
                entities['concepts'].append(term)
        
        return entities
    
    def _extract_key_terms(self, query: str) -> List[str]:
        """
        Extrae t√©rminos clave de la query
        
        Args:
            query (str): Query a analizar
            
        Returns:
            List[str]: T√©rminos clave ordenados por importancia
        """
        stopwords = {
            'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'es', 'se', 'no', 'te', 'lo', 'le',
            'como', 'sobre', 'seg√∫n', 'para', 'con', 'por', 'sin', 'hasta', 'desde'
        }
        
        words = re.findall(r'\b\w+\b', query.lower())
        key_terms = [word for word in words if word not in stopwords and len(word) > 2]
        
        # Priorizar t√©rminos acad√©micos
        academic_priority = [term for term in key_terms if term in self.academic_terms]
        other_terms = [term for term in key_terms if term not in self.academic_terms]
        
        return academic_priority + other_terms
    
    def _generate_suggestions(self, query: str) -> List[str]:
        """
        Genera sugerencias para mejorar la query
        
        Args:
            query (str): Query original
            
        Returns:
            List[str]: Lista de sugerencias
        """
        suggestions = []
        
        if len(query.split()) < 3:
            suggestions.append("Considera agregar m√°s t√©rminos espec√≠ficos para obtener mejores resultados")
        
        if not any(term in query for term in self.academic_terms):
            suggestions.append("Puedes incluir t√©rminos acad√©micos como 'an√°lisis', 'estudio', 'teor√≠a'")
        
        if not re.search(r'\b(19|20)\d{2}\b', query):
            suggestions.append("Si buscas informaci√≥n hist√≥rica, incluye a√±os espec√≠ficos")
        
        return suggestions
    
    def _calculate_academic_score(self, query: str) -> float:
        """
        Calcula score acad√©mico de la query
        
        Args:
            query (str): Query a evaluar
            
        Returns:
            float: Score entre 0 y 1
        """
        score = 0.0
        factors = 0
        
        # Factor 1: Longitud apropiada
        word_count = len(query.split())
        if 3 <= word_count <= 10:
            score += 0.3
        factors += 0.3
        
        # Factor 2: Presencia de t√©rminos acad√©micos
        academic_count = sum(1 for term in self.academic_terms if term in query)
        if academic_count > 0:
            score += min(0.4, academic_count * 0.2)
        factors += 0.4
        
        # Factor 3: Especificidad
        if any(re.search(pattern, query) for pattern in [r'\b(19|20)\d{2}\b', r'[A-Z][a-z]+']):
            score += 0.3
        factors += 0.3
        
        return score / factors if factors > 0 else 0.0

def validate_embeddings_consistency(embeddings_list: List[List[float]]) -> Dict[str, Any]:
    """
    Valida consistencia de embeddings generados
    
    VERIFICACIONES:
    - Dimensionalidad consistente (768 para all-mpnet-base-v2)
    - Rango de valores apropiado
    - Ausencia de NaN o infinitos
    - Distribuci√≥n estad√≠stica normal
    
    Args:
        embeddings_list (List[List[float]]): Lista de embeddings a validar
        
    Returns:
        Dict: Reporte de validaci√≥n
    """
    import numpy as np
    
    if not embeddings_list:
        return {'valid': False, 'error': 'Lista de embeddings vac√≠a'}
    
    try:
        # Convertir a array numpy para an√°lisis
        embeddings_array = np.array(embeddings_list)
        
        # === VALIDACI√ìN 1: DIMENSIONALIDAD ===
        expected_dim = 768  # all-mpnet-base-v2
        actual_dims = [len(emb) for emb in embeddings_list]
        
        if not all(dim == expected_dim for dim in actual_dims):
            return {
                'valid': False,
                'error': f'Dimensiones inconsistentes. Esperado: {expected_dim}, Encontrado: {set(actual_dims)}'
            }
        
        # === VALIDACI√ìN 2: VALORES NUM√âRICOS ===
        if np.any(np.isnan(embeddings_array)):
            return {'valid': False, 'error': 'Embeddings contienen valores NaN'}
        
        if np.any(np.isinf(embeddings_array)):
            return {'valid': False, 'error': 'Embeddings contienen valores infinitos'}
        
        # === VALIDACI√ìN 3: RANGO DE VALORES ===
        min_val = np.min(embeddings_array)
        max_val = np.max(embeddings_array)
        
        # Para all-mpnet-base-v2, valores t√≠picos est√°n en [-1, 1]
        if min_val < -2 or max_val > 2:
            return {
                'valid': False,
                'error': f'Rango de valores sospechoso: [{min_val:.3f}, {max_val:.3f}]'
            }
        
        # === VALIDACI√ìN 4: ESTAD√çSTICAS ===
        mean_val = np.mean(embeddings_array)
        std_val = np.std(embeddings_array)
        
        # === REPORTE DE VALIDACI√ìN ===
        validation_report = {
            'valid': True,
            'embedding_count': len(embeddings_list),
            'dimensions': expected_dim,
            'value_range': [float(min_val), float(max_val)],
            'mean': float(mean_val),
            'std': float(std_val),
            'total_elements': embeddings_array.size
        }
        
        logger.info(f"‚úÖ Embeddings validados: {len(embeddings_list)} vectores, {expected_dim}D")
        return validation_report
        
    except Exception as e:
        return {'valid': False, 'error': f'Error en validaci√≥n: {str(e)}'}

def measure_embedding_performance(func):
    """
    Decorador para medir performance de funciones de embeddings
    
    Args:
        func: Funci√≥n a medir
        
    Returns:
        function: Funci√≥n decorada con medici√≥n de performance
    """
    def wrapper(*args, **kwargs):
        start_time = time.time()
        
        # Ejecutar funci√≥n
        result = func(*args, **kwargs)
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Log de performance
        logger.info(f"‚è±Ô∏è {func.__name__} ejecutado en {execution_time:.2f}s")
        
        return result
    
    return wrapper

def create_backup_embeddings(collection, backup_path: str = "backup_embeddings"):
    """
    Crea backup de embeddings de ChromaDB
    
    Args:
        collection: Colecci√≥n de ChromaDB
        backup_path (str): Ruta para el backup
    """
    try:
        backup_dir = Path(backup_path)
        backup_dir.mkdir(exist_ok=True)
        
        # Obtener todos los datos
        all_data = collection.get(include=['embeddings', 'documents', 'metadatas'])
        
        # Crear backup con timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = backup_dir / f"embeddings_backup_{timestamp}.json"
        
        # Guardar datos (embeddings son muy grandes, considerar compresi√≥n)
        backup_data = {
            'timestamp': timestamp,
            'collection_name': collection.name,
            'document_count': len(all_data.get('documents', [])),
            'documents': all_data.get('documents', []),
            'metadatas': all_data.get('metadatas', []),
            # Nota: embeddings no incluidos por tama√±o, solo metadata
        }
        
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(backup_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üíæ Backup creado: {backup_file}")
        return str(backup_file)
        
    except Exception as e:
        logger.error(f"‚ùå Error creando backup: {e}")
        return None

# === UTILIDADES DE LOGGING AVANZADO ===

class RAGLogger:
    """
    Logger especializado para operaciones RAG
    """
    
    def __init__(self, name: str = "RAG_System"):
        self.logger = logging.getLogger(name)
        self.setup_logger()
    
    def setup_logger(self):
        """
        Configura logger con formato especializado para RAG
        """
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s | %(name)s | %(levelname)s | %(message)s',
                datefmt='%H:%M:%S'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
    
    def log_search_operation(self, query: str, results_count: int, execution_time: float):
        """
        Log especializado para operaciones de b√∫squeda
        """
        self.logger.info(
            f"üîç B√öSQUEDA | Query: '{query[:50]}...' | "
            f"Resultados: {results_count} | Tiempo: {execution_time:.2f}s"
        )
    
    def log_embedding_operation(self, text_count: int, embedding_dim: int, execution_time: float):
        """
        Log especializado para generaci√≥n de embeddings
        """
        self.logger.info(
            f"ü§ñ EMBEDDINGS | Textos: {text_count} | "
            f"Dimensiones: {embedding_dim} | Tiempo: {execution_time:.2f}s"
        )
    
    def log_llm_operation(self, model: str, tokens_used: int, execution_time: float):
        """
        Log especializado para operaciones LLM
        """
        self.logger.info(
            f"üí¨ LLM | Modelo: {model} | "
            f"Tokens: {tokens_used} | Tiempo: {execution_time:.2f}s"
        )

# Instancia global del logger RAG
rag_logger = RAGLogger()